{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP - Preparing The Data/Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, the NLP Cleaning code is copied here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in the data\n",
    "\n",
    "import pandas as pd\n",
    "train = pd.read_csv('/Users/helenabelloff/Desktop/NLP/train.csv')\n",
    "train.head()\n",
    "\n",
    "# Remove Hyperlinks\n",
    "\n",
    "def hyperlink_remove(text):\n",
    "    hyperlink_remove = train['text'].str.replace('http\\S+|http.\\S+', '', case=False)\n",
    "    return hyperlink_remove\n",
    "\n",
    "train['hyperlink_remove'] = hyperlink_remove(train['text'])\n",
    "\n",
    "# Removing punctuation\n",
    "\n",
    "import string\n",
    "string.punctuation\n",
    "\n",
    "def remove_punct(text):\n",
    "    nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    return nopunct\n",
    "\n",
    "train['text_no_punct'] = train['hyperlink_remove'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "# Remove digits\n",
    "\n",
    "import string\n",
    "\n",
    "train['text_no_punct'] = train['text_no_punct'].str.replace(r\"\\d\",\"\", regex= True)\n",
    "\n",
    "# Text to lower\n",
    "\n",
    "def text_lower(text):\n",
    "    text = text.lower()\n",
    "    return text\n",
    "train['text_no_punct_lower'] = train['text_no_punct'].apply(lambda x: text_lower(x))\n",
    "\n",
    "# Getting rid of accents etc.\n",
    "\n",
    "import unicodedata\n",
    "train['text_no_punct_lower'] = train['text_no_punct_lower'].str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "\n",
    "# Trying to standardize words\n",
    "\n",
    "from itertools import product\n",
    "import itertools\n",
    "\n",
    "train['text_no_punct_lower'] = train['text_no_punct_lower'].apply(lambda x:''.join(''.join(s)[:2] for _, s in itertools.groupby(x)))\n",
    "\n",
    "# Tokenizing\n",
    "\n",
    "import re\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = re.split('\\W+', text)\n",
    "    return tokens\n",
    "\n",
    "train['tokenized_text'] = train['text_no_punct_lower'].apply(lambda x: tokenize(x.lower()))\n",
    "\n",
    "stopword = {'ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', \n",
    "            'once', 'during', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', \n",
    "            'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', \n",
    "            'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', \n",
    "            'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', \n",
    "            'don', 'nor', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', \n",
    "            'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', \n",
    "            'no', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', \n",
    "            'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', \n",
    "            'so', 'can', 'did', 'not', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', \n",
    "            'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', \n",
    "            'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', \n",
    "            'was', 'here', 'than', 'amp', 'wa', 'am', 'pm', 'im', 'leh', 'ind', 'inciweb', 'ina'}\n",
    "\n",
    "def remove_stopwords(tokenized_list):\n",
    "    text = [word for word in tokenized_list if word not in stopword]\n",
    "    return text\n",
    "\n",
    "train['tokenized_nostopwords'] = train['tokenized_text'].apply(lambda x: remove_stopwords(x))\n",
    "\n",
    "# Need to remove stems like \"ing\" and \"ly\" - destemming\n",
    "\n",
    "import nltk\n",
    "\n",
    "port = nltk.PorterStemmer()\n",
    "\n",
    "def stemming(tokenized_text):\n",
    "    text = [port.stem(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "train['tokenized_destemmed'] = train['tokenized_nostopwords'].apply(lambda x: stemming(x))\n",
    "\n",
    "# Lemmatizing\n",
    "\n",
    "import nltk\n",
    "\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def lemmatizing(tokenized_text):\n",
    "    text = [lemmatizer.lemmatize(word) for word in tokenized_text]\n",
    "    return text\n",
    "\n",
    "train['token_lemmatized'] = train['tokenized_nostopwords'].apply(lambda x: lemmatizing(x))\n",
    "\n",
    "# Joining Text\n",
    "\n",
    "def join_text(text):\n",
    "    text = ' '.join(text)\n",
    "    return text\n",
    "train['token_lemmatized'] = train['token_lemmatized'].apply(lambda x: join_text(x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preparing the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define body\n",
    "\n",
    "x = train['token_lemmatized']\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels\n",
    "\n",
    "y = train['target']\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words using Tfidf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score,train_test_split,GridSearchCV\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Checking word count\n",
    "word_count = cv.fit_transform(x)\n",
    "#print(word_count)\n",
    "\n",
    "# Ok, word count shape checks out\n",
    "#print(word_count.shape)\n",
    "\n",
    "# Now, compute IDF values\n",
    "tfidf_trans = TfidfTransformer(smooth_idf = True, use_idf = True)\n",
    "tfidf_trans.fit(word_count)\n",
    "\n",
    "# Print idf values\n",
    "idf_df = pd.DataFrame(tfidf_trans.idf_, index = cv.get_feature_names(), columns = [\"idf_weights\"])\n",
    " \n",
    "# Sort ascending\n",
    "idf_df.sort_values(by = ['idf_weights'])\n",
    "\n",
    "count_vector = cv.transform(x)\n",
    " \n",
    "# tf-idf scores\n",
    "# Computing the tf * idf  multiplication where term frequency is weighted by its IDF values\n",
    "tfidf_vector = tfidf_trans.transform(count_vector)\n",
    "\n",
    "\n",
    "feature_names = cv.get_feature_names()\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X1 = cv.fit_transform(x)\n",
    "X = tfidf.fit_transform(x)\n",
    "print(X.shape)\n",
    "\n",
    "train_df = pd.DataFrame(columns = feature_names, data = X.toarray())\n",
    "print(train_df.shape)\n",
    "print(train_df.head(10))\n",
    "\n",
    "# Splitting the data into train and test (validation)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,stratify=y,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Final Data Frame\n",
    "\n",
    "train.reset_index(inplace=True, drop = True)\n",
    "train.drop(['target','text', 'text_no_punct', 'tokenized_text', 'hyperlink_remove', 'tokenized_nostopwords', \n",
    "            'tokenized_destemmed', 'token_lemmatized', 'text_no_punct_lower'],axis = 1, inplace = True)\n",
    "train = pd.concat([train,train_df], axis =1)\n",
    "print(train.shape)\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['keyword'] = train['keyword'].fillna('NA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a feature for the length of the tweet and the percentage of punctuation in the tweet\n",
    "\n",
    "import string\n",
    "\n",
    "train['tweet_length'] = train['text'].apply(lambda x: len(x) - x.count(\" \"))\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_punct(text):\n",
    "    count = sum([1 for char in text if char in string.punctuation])\n",
    "    return round(count/(len(text) - text.count(\" \")), 3)*100\n",
    "\n",
    "train['tweet_punct_percent'] = train['text'].apply(lambda x: count_punct(x))\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's plot these features\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.linspace(0, 200, 40)\n",
    "\n",
    "plt.hist(train[train['target']==1]['tweet_length'], bins, alpha = 0.5, normed = True, label = 'Real Disaster Tweet')\n",
    "plt.hist(train[train['target']==0]['tweet_length'], bins, alpha = 0.5, normed = True, label = 'Fake Disaster Tweet')\n",
    "plt.legend(loc = 'upper left')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Tweet Length')\n",
    "plt.suptitle('Histogram of Tweet Length')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "bins = np.linspace(0, 50, 40)\n",
    "\n",
    "plt.hist(train[train['target']==1]['tweet_punct_percent'], bins, alpha = 0.5, normed = True, label = 'Real Disaster Tweet')\n",
    "plt.hist(train[train['target']==0]['tweet_punct_percent'], bins, alpha = 0.5, normed = True, label = 'Fake Disaster Tweet')\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('% of Punctuation in Tweet')\n",
    "plt.suptitle('Histogram of the Percent of Punctuation in Tweet')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Word Clouds**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#git clone https://github.com/amueller/word_cloud.git\n",
    "#cd word_cloud\n",
    "#pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Combining all texts for the Real Disaster Tweets only\n",
    "text = \" \".join(review for review in train[train['target']==1]['text_no_punct'])\n",
    "print (\"There are {} words in real Disaster Tweets.\".format(len(train[train['target']==1]['text_no_punct'])))\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Combining all texts for the Fake Disaster Tweets only\n",
    "text = \" \".join(review for review in train[train['target']==0]['text_no_punct'])\n",
    "print (\"There are {} words in Fake Disaster Tweets.\".format(len(train[train['target']==0]['text_no_punct'])))\n",
    "\n",
    "wordcloud = WordCloud(background_color=\"white\").generate(text)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make these in the shape of the twitter logo !\n",
    "\n",
    "# Will be all 0's\n",
    "twitter_mask = np.array(Image.open(\"/Users/helenabelloff/Desktop/Twitter.png\"))\n",
    "twitter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_twitter(val):\n",
    "    if val == 0:\n",
    "        return 255\n",
    "    else:\n",
    "        return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as no\n",
    "\n",
    "transformed_twitter_mask = np.ndarray((twitter_mask.shape[0], twitter_mask.shape[1]), np.int32)\n",
    "\n",
    "for i in range(len(twitter_mask)):\n",
    "    transformed_twitter_mask[i] = list(map(transform_twitter, twitter_mask[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that it's now all 255's\n",
    "\n",
    "transformed_twitter_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we redo for our shape!\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Combining all texts for the Real Disaster Tweets only\n",
    "text = \" \".join(review for review in train[train['target']==1]['text_no_punct'])\n",
    "print (\"There are {} words in Real Disaster Tweets.\".format(len(train[train['target']==1]['text_no_punct'])))\n",
    "\n",
    "wordcloud = WordCloud(stopwords = stopword, background_color = \"white\", mask = transformed_twitter_mask).generate(text)\n",
    "\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "# Combining all texts for the Fake Disaster Tweets only\n",
    "text = \" \".join(review for review in train[train['target']==0]['text_no_punct'])\n",
    "print (\"There are {} words in Fake Disaster Tweets.\".format(len(train[train['target']==0]['text_no_punct'])))\n",
    "\n",
    "wordcloud = WordCloud(stopwords = stopword, background_color = \"white\", mask = transformed_twitter_mask).generate(text)\n",
    "\n",
    "plt.figure(figsize=[20,10])\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
